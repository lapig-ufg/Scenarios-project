{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33553e4-eb1e-4da9-959a-fa978dc62ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import libraries\n",
    "import os\n",
    "import time\n",
    "import cupy as cp\n",
    "import numpy as np # We still need numpy for CPU operations with Pandas\n",
    "import pandas as pd\n",
    "from cupyx.scipy.ndimage import binary_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffc0c9-089c-4883-b317-749b26cbc532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório definido: E:\\Process\n"
     ]
    }
   ],
   "source": [
    "# 3. Define paths and structure\n",
    "base_dir = r'E:\\Process'\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(f\"The directory {base_dir} does not exist.\")\n",
    "else:\n",
    "    print(f\"Directory set: {base_dir}\")\n",
    "\n",
    "# Structure for binary dilation - stays on GPU\n",
    "structure = cp.array([[0, 1, 0],\n",
    "                      [1, 1, 1],\n",
    "                      [0, 1, 0]], dtype=cp.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7ee98-a5c8-444a-95b8-7d7d66871c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Optimized processing functions\n",
    "def analyze_neighborhood_gpu_vectorized(patches_A_gpu, patches_B_gpu):\n",
    "    print(\"Starting neighborhood analysis on GPU (vectorized)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get unique patch IDs in patches_A, ignoring 0\n",
    "    patch_ids_A = cp.unique(patches_A_gpu[patches_A_gpu > 0])\n",
    "    total_patches = len(patch_ids_A)\n",
    "    print(f\"Total unique patches to analyze: {total_patches}\")\n",
    "\n",
    "    # Initialize arrays to store results on the GPU\n",
    "    results_patch_id = cp.empty(total_patches, dtype=cp.int32)\n",
    "    results_vizinhanza = cp.empty(total_patches, dtype=cp.uint8)\n",
    "   \n",
    "    max_patch_id_A = int(patch_ids_A.max()) # Convert to int to avoid indexing issues\n",
    "    # Map to store results\n",
    "    vizinhanza_map = cp.zeros(max_patch_id_A + 1, dtype=cp.uint8)\n",
    "\n",
    "    # Process in blocks to manage GPU memory and progress\n",
    "    GPU_BLOCK_SIZE = 1000 # Adjust this value based on GPU memory and number of patches\n",
    "\n",
    "    for i in range(0, total_patches, GPU_BLOCK_SIZE):\n",
    "        block_ids = patch_ids_A[i : min(i + GPU_BLOCK_SIZE, total_patches)]\n",
    "        print(f\"  Processing patch block {i+1} to {min(i + GPU_BLOCK_SIZE, total_patches)} of {total_patches} on GPU...\") \n",
    "    all_patch_ids_A = cp.unique(patches_A_gpu[patches_A_gpu > 0])\n",
    "    total_unique_patches = len(all_patch_ids_A)\n",
    "\n",
    "    # Prepare arrays for results, already on the GPU\n",
    "    results_patch_ids = cp.empty(total_unique_patches, dtype=cp.int32)\n",
    "    results_vizinhanza = cp.empty(total_unique_patches, dtype=cp.uint8)\n",
    "\n",
    "       # This is the main optimization point for your case.\n",
    "    for i, patch_id_val in enumerate(all_patch_ids_A):\n",
    "        # The operations below are all CuPy and run on the GPU\n",
    "        mask_A = patches_A_gpu == patch_id_val\n",
    "        dilated = binary_dilation(mask_A, structure=structure)\n",
    "        border = cp.logical_and(dilated, ~mask_A)\n",
    "\n",
    "        neighbor_values = patches_B_gpu[border]\n",
    "        neighbor_ids = cp.unique(neighbor_values[neighbor_values > 0])\n",
    "\n",
    "        # Neighborhood logic (still serialized by patch_id, but running fast on the GPU) \n",
    "        viz = 0 # Initialize with a default value in case no condition is met\n",
    "        if len(neighbor_ids) == 0:\n",
    "            viz = 1\n",
    "        elif len(neighbor_ids) == 1:\n",
    "            # Note: cp.any and cp.all in CuPy operate on CuPy arrays.\n",
    "            # If neighbor_values is a CuPy array, cp.any/cp.all will work.\n",
    "            if cp.any(neighbor_values == 0): # Check if there is any \"stable\" pixel (value 0) in the neighborhood\n",
    "                viz = 2\n",
    "            elif cp.all(neighbor_values == neighbor_ids[0]): # Check if all neighbor pixels have the same ID\n",
    "                viz = 4\n",
    "            else: # Different IDs in the neighborhood, but only one unique neighbor_id (e.g., 0 and another ID)\n",
    "                viz = 2\n",
    "        else: # More than one neighbor_id\n",
    "            viz = 3\n",
    "\n",
    "        results_patch_ids[i] = patch_id_val\n",
    "        results_vizinhanza[i] = viz\n",
    "\n",
    "        if (i + 1) % 1000 == 0: # Progress feedback \n",
    "            print(f\"  Processed {i+1} of {total_unique_patches} patches...\")\n",
    "            cp.cuda.Stream.null.synchronize() # Ensure GPU operations are completed for accurate timing\n",
    "\n",
    "    # Transfer the final results from GPU to CPU ONLY ONCE\n",
    "    final_patch_ids = cp.asnumpy(results_patch_ids)\n",
    "    final_vizinhanza = cp.asnumpy(results_vizinhanza)\n",
    "\n",
    "    print(f\"Neighborhood analysis completed in {time.time() - start_time:.2f} seconds.\")\n",
    "    return list(zip(final_patch_ids, final_vizinhanza))\n",
    "\n",
    "\n",
    "# 5. Check GPU (remains the same)\n",
    "print(\"GPU available:\", cp.cuda.Device(0).attributes.get('name', 'Name not available'))\n",
    "cp.cuda.Device(0).use() # Ensure the default GPU is used\n",
    "\n",
    "# 6. Load .npy files (remains the same, load directly to GPU)\n",
    "print(\"Loading .npy files to GPU...\")\n",
    "ganho = cp.load(os.path.join(base_dir, 'ganho_patches_2013_2018.npy'))\n",
    "perda = cp.load(os.path.join(base_dir, 'perda_patches_2013_2018.npy'))\n",
    "estavel = cp.load(os.path.join(base_dir, 'estavel_patches_2013_2018.npy'))\n",
    "print(\"Files loaded to GPU.\")\n",
    "# 7. Execute analysis with the optimized function\n",
    "print(\"\\nAnalyzing loss...\")\n",
    "resultado_perda = analisar_vizinhança_gpu_vetorizado(perda, estavel)\n",
    "\n",
    "print(\"\\nAnalyzing gain...\")\n",
    "resultado_ganho = analisar_vizinhança_gpu_vetorizado(ganho, estavel)\n",
    "\n",
    "\n",
    "# 8. Save results to CSV (remains the same)\n",
    "print(\"\\nSaving results to CSV...\")\n",
    "df_perda = pd.DataFrame(resultado_perda, columns=['patch_id', 'neighborhood'])\n",
    "df_ganho = pd.DataFrame(resultado_ganho, columns=['patch_id', 'neighborhood'])\n",
    "\n",
    "# Check if the output file name is correct (was 1985_1986, now 1986_1987)\n",
    "df_perda.to_csv(os.path.join(base_dir, 'resultado_neighborhood_loss_2013_2018.csv'), index=False)\n",
    "df_ganho.to_csv(os.path.join(base_dir, 'resultado_neighborhood_gain_2013_2018.csv'), index=False)\n",
    "\n",
    "print(\"Results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7154c85a-7f2b-462b-8a90-64a65f77cd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: affine in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (23.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (2025.4.26)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (1.26.4)\n",
      "Requirement already satisfied: click-plugins in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from rasterio) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mario.barroso\\appdata\\local\\anaconda3\\lib\\site-packages (from click>=4.0->rasterio) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906cde1-66d3-46cb-9b03-0673ac15fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando conversão: resultado_vizinhanca_perda_2015_2016.csv\n",
      "Salvando GeoTIFF...\n",
      "GeoTIFF salvo: E:\\Process\\vizinhanca_perda_2015_2016.tif\n",
      "\n",
      "Iniciando conversão: resultado_vizinhanca_ganho_2015_2016.csv\n",
      "Salvando GeoTIFF...\n",
      "GeoTIFF salvo: E:\\Process\\vizinhanca_ganho_2015_2016.tif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.enums import Compression\n",
    "from rasterio.errors import RasterioIOError #IMPORT TO HANDLE I/O ERRORS\n",
    "\n",
    "# Base directory on Google Drive \n",
    "base_dir = r'E:\\Process'\n",
    "\n",
    "# Raster parameters\n",
    "# Consistency in variable names (camelCase to snake_case, by Python convention)\n",
    "COLS = 12989\n",
    "ROWS = 9278\n",
    "CELL_SIZE_X = 0.0002694945852358565\n",
    "CELL_SIZE_Y = 0.00026949458523585663\n",
    "UPPER_LEFT_X = -55.500254\n",
    "UPPER_LEFT_Y = -16.499806\n",
    "\n",
    "# Geographic transformation\n",
    "transform = from_origin(UPPER_LEFT_X, UPPER_LEFT_Y, CELL_SIZE_X, CELL_SIZE_Y)\n",
    "\n",
    "# Coordinate reference system WGS 84 (EPSG:4326)\n",
    "CRS = 'EPSG:4326'\n",
    "\n",
    "# Optimized function to convert neighborhood CSV to GeoTIFF using patch reference\n",
    "def neighborhood_to_geotiff_optimized(csv_path, npy_patch_path, output_tif_path):\n",
    "    print(f\"Starting conversion: {os.path.basename(csv_path)}\")\n",
    "\n",
    "    try:\n",
    "        # Load CSV: usecols to load only necessary columns, reducing memory consumption\n",
    "        df = pd.read_csv(csv_path, usecols=['patch_id', 'neighborhood'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {csv_path}\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: CSV file is empty or malformed at {csv_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load patches\n",
    "        patches = np.load(npy_patch_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: NPY file not found at {npy_patch_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NPY file {npy_patch_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ensure that the dimensions of patches match the expected ones\n",
    "    if patches.shape != (ROWS, COLS):\n",
    "        print(f\"Warning: Dimensions of NPY file ({patches.shape}) do not match the expected ({ROWS}, {COLS}).\")\n",
    "        # You may choose to resize or raise an error depending on desired behavior\n",
    "        # Ex: patches = np.resize(patches, (ROWS, COLS))\n",
    "\n",
    "    # Initialize the output raster with zeros, using the same dtype as neighborhood values or np.uint8\n",
    "    # If 'neighborhood' can have values outside uint8, adjust the dtype (e.g., np.int16)\n",
    "    # It is crucial that raster_out has the same data type as the output GeoTIFF for efficiency\n",
    "    raster_out = np.zeros_like(patches, dtype=np.uint8) # Assuming 'neighborhood' fits in uint8\n",
    "\n",
    "    # Main optimization: Vectorized mapping\n",
    "    # 1. Create an auxiliary array to map patch_id to neighborhood\n",
    "    # Determine the maximum patch_id value to size the mapping array\n",
    "    max_patch_id = df['patch_id'].max()\n",
    "    if max_patch_id >= (2**32 - 1): # If patch_id is very large, using a dictionary might be better\n",
    "        print(\"Warning: patch_id is very large, using dictionary for mapping.\")\n",
    "        patch_id_to_neighborhood = dict(zip(df['patch_id'], df['neighborhood']))\n",
    "        # Iterate over patches and apply mapping\n",
    "        for i in np.unique(patches): # Iterate over unique patch IDs present in the raster\n",
    "            if i in patch_id_to_neighborhood:\n",
    "                raster_out[patches == i] = patch_id_to_neighborhood[i]\n",
    "    else:\n",
    "        # Create a lookup array where the index is the patch_id and the value is the neighborhood\n",
    "        # Initialize with 0 (or an appropriate 'no data' value)\n",
    "        lookup_table = np.zeros(max_patch_id + 1, dtype=np.uint8)\n",
    "        # Fill the lookup table with neighborhood values\n",
    "        lookup_table[df['patch_id'].values] = df['neighborhood'].values\n",
    "\n",
    "        # Apply the vectorized mapping: for each pixel in 'patches', use its value as an index into the lookup_table\n",
    "        # This is SIGNIFICANTLY faster than the original loop.\n",
    "        raster_out = lookup_table[patches]\n",
    "\n",
    "    print(\"Saving GeoTIFF...\")\n",
    "    try:\n",
    "        with rasterio.open(\n",
    "            output_tif_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=ROWS,\n",
    "            width=COLS,\n",
    "            count=1,\n",
    "            dtype=raster_out.dtype, # Use the dtype of the output array\n",
    "            crs=CRS,\n",
    "            transform=transform,\n",
    "            compress=Compression.lzw, # LZW compression is lossless and efficient\n",
    "            tiled=True,              # Enable tiling for better read/write performance on large files\n",
    "            blockxsize=256,          # Block size for tiling (adjust if necessary)\n",
    "            blockysize=256           # Block size for tiling (adjust if necessary)\n",
    "        ) as dst:\n",
    "            dst.write(raster_out, 1)\n",
    "        print(f\"GeoTIFF saved: {output_tif_path}\\n\")\n",
    "    except RasterioIOError as e:\n",
    "        print(f\"I/O error while saving GeoTIFF {output_tif_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while saving GeoTIFF {output_tif_path}: {e}\")\n",
    "# File paths \n",
    "csv_perda = os.path.join(base_dir, 'resultado_vizinhanca_perda_2013_2018.csv')\n",
    "csv_ganho = os.path.join(base_dir, 'resultado_vizinhanca_ganho_2013_2018.csv')\n",
    "npy_perda = os.path.join(base_dir, 'perda_patches_2013_2018.npy')\n",
    "npy_ganho = os.path.join(base_dir, 'ganho_patches_2013_2018.npy')\n",
    "tif_perda = os.path.join(base_dir, 'vizinhanca_perda_2013_2018.tif')\n",
    "tif_ganho = os.path.join(base_dir, 'vizinhanca_ganho_2013_2018.tif')\n",
    "\n",
    "# Execute conversion with the optimized function\n",
    "vizinhanca_para_geotiff_otimizado(csv_perda, npy_perda, tif_perda)\n",
    "vizinhanca_para_geotiff_otimizado(csv_ganho, npy_ganho, tif_ganho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957b9cb-cd57-4640-8530-453ccbfdfd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
